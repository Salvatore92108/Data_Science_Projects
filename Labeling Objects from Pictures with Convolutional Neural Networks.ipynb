{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3c75f3",
   "metadata": {},
   "source": [
    "# Labeling Objects from Pictures with Convolutional Neural Networks\n",
    "\n",
    "### Author: Salvatore Porcheddu\n",
    "### Date: 19/05/2022\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a practice computer vision project where we will try to build a **Convolutional Neural Network (CNN)** that can correctly label what is in the pictures in the famous **CIFAR-10** dataset with at least 75% accuracy. \n",
    "\n",
    "The **CIFAR-10** dataset contains a total of 60,000 images belonging to ten different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. \n",
    "While it can be downloaded directly in a suitable format within the deep learning library *Keras* (which is part of *TensorFlow*), in this project we will download it from the [original source](https://www.cs.toronto.edu/~kriz/cifar.html) and make it suitable for proper use with *Keras* ourselves.\n",
    "\n",
    "To learn more about the CIFAR-10 dataset please read \"**Learning Multiple Layers of Features from Tiny Images**\", published by **Alex Krizhevsky** in 2009, that can be found [here](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef6d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages, deactivating warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras import models, layers, preprocessing, callbacks, utils\n",
    "from tensorflow import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414053b2",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The CIFAR-10 dataset comes in six serialized batches of 10,000 images each; five of them are to be used for training and the other one for testing.\n",
    "\n",
    "Before we can do anything with the data, we will need to deserialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to load serialized files containing the images\n",
    "\n",
    "def deserializer(filename):\n",
    "    \"\"\"This function takes a filename, opens the corresponding file, \n",
    "    performs deserialization and finally returns the deserialized file.\n",
    "    \n",
    "    Args:\n",
    "    filename: string corresponding to the name of the file to be deserialized.\n",
    "    \n",
    "    Returns:\n",
    "    deserialized file.\"\"\"\n",
    "    \n",
    "    with open(filename, \"rb\") as f:\n",
    "        file = pickle.load(f, encoding=\"bytes\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc2646d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining file names\n",
    "\n",
    "batch_names = [f\"data_batch_{n}\" for n in range(1, 6)]\n",
    "labs = \"batches.meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e630f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files and labels\n",
    "\n",
    "batches = [deserializer(b) for b in batch_names]\n",
    "\n",
    "labels = deserializer(labs)\n",
    "\n",
    "test = deserializer(\"test_batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6e2fd",
   "metadata": {},
   "source": [
    "Each of the batches is a dictionary containing arrays for each picture and the corresponding labels. We will later merge the batches together and let *Keras* do the batching work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa96636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining the batches\n",
    "\n",
    "batches[0][b\"data\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde02792",
   "metadata": {},
   "source": [
    "As the [documentation](https://www.cs.toronto.edu/~kriz/cifar.html) explains, there are 10,000 32x32 pictures, with 1024 entries for each of the RGB channels. \n",
    "The image is stored in row-major order, which means that the rgb channels are ordered by row (each row contains data related to one of the channels).\n",
    "The data needs to be reshaped in order to be correctly fed to a *Keras* convolutional neural network. \n",
    "\n",
    "The reshaping process will entail the following steps:\n",
    "\n",
    "- Splitting the data thereby separating the three channels. \n",
    "- Reshaping each channel data to the image shape of 32x32.\n",
    "- Restacking the channel data together making sure that it is the right format (each element, which corresponds to a pixel, must have three values, one for the red channel, one for the green channel and one for the blue channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f729a40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10000, 3072)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the arrays and creating a validation dataset\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for b in batches:\n",
    "    if b != batches[-1]:\n",
    "        X_train.append(b[b\"data\"])\n",
    "        y_train.append(b[b\"labels\"])\n",
    "    else:\n",
    "        X_val.append(b[b\"data\"])\n",
    "        y_val.append(b[b\"labels\"])\n",
    "    \n",
    "X_train, y_train = np.array(X_train, dtype=\"uint8\"), np.array(y_train, dtype=\"uint8\")\n",
    "X_val, y_val = np.array(X_val, dtype=\"uint8\"), np.array(y_val, dtype=\"uint8\") \n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0cc3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that reshapes the data to the correct format\n",
    "\n",
    "def rgb_reshaper(data, n_batches=1):\n",
    "    \"\"\"This function takes the deserialized image data from the CIFAR-10 dataset and performs array \n",
    "    transformations to convert it into rgb data.\n",
    "    \n",
    "    Args:\n",
    "    data: numpy array containing image data.\n",
    "    n_batches: integer referring to the number of the original CIFAR-10 batches that 'data' contains. \n",
    "               This value will be used for reshaping 'data' and defaults to 1.\n",
    "               \n",
    "    Returns:\n",
    "    Numpy array with rgb data.\"\"\"\n",
    "    \n",
    "    r, g, b = np.split(data, 3, axis=2) # splits the data into separate arrays for each channel\n",
    "    r = r.reshape((10000*n_batches, 32, 32))\n",
    "    g = g.reshape((10000*n_batches, 32, 32))\n",
    "    b = b.reshape((10000*n_batches, 32, 32))\n",
    "    rgb = np.stack([r, g, b], axis=3) # stacks the channel arrays together\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0db80d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# executing the reshapes for training and validation data\n",
    "\n",
    "X_train = rgb_reshaper(X_train, 4)\n",
    "X_val = rgb_reshaper(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49014c4",
   "metadata": {},
   "source": [
    "Reshaping the data in this way allows us to visualize the images with *matplotlib*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5048266",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[27])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0677e2c",
   "metadata": {},
   "source": [
    "The labels need less reshaping work but they need to be transformed into binary class matrices so that *Keras* will correctly recognize that there are ten different classes when we train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd0bd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = utils.to_categorical(y_train.reshape((10000*4)), 10) \n",
    "y_val = utils.to_categorical(y_val.reshape((10000)), 10)\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe11c681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the test dataset\n",
    "\n",
    "X_test = rgb_reshaper(test[b\"data\"].reshape((1, 10000, -1)))\n",
    "y_test = utils.to_categorical(np.array(test[b\"labels\"], dtype=\"uint8\"), 10)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2bde1",
   "metadata": {},
   "source": [
    "## Building and training the network\n",
    "\n",
    "Before building the network, we will perform three preliminary steps:\n",
    "\n",
    "- Setting *random seeds* so that every time that we run the code we get the same results.\n",
    "- Building an image data generator that takes our images and performs manipulation of various kinds (e.g. rotation, zooming...) in order to generate more images to be used together with the original ones for training and validation. This method is very useful so that our model does not *overfit* (i.e. memorizes the training data without being able to generalize to new and unseen data).\n",
    "- Creating an early stopping *callback*, which allows us to stop the training of the network if performance is not improving within a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a1944bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seeds\n",
    "\n",
    "np.random.seed(27)\n",
    "random.set_seed(27)\n",
    "\n",
    "# augmenting training and validation data\n",
    "\n",
    "generator = preprocessing.image.ImageDataGenerator(rotation_range=.2, zoom_range=.2, shear_range=.2, horizontal_flip=True)\n",
    "aug_train = generator.flow(X_train, y_train)\n",
    "aug_val = generator.flow(X_val, y_val)\n",
    "\n",
    "# creating a callback to stop training if there are no improvements in the validation accuracy within 20 consecutive epochs\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2c0dd",
   "metadata": {},
   "source": [
    "Now we can proceed to build, compile and train a CNN. \n",
    "\n",
    "The training will last 100 epochs unless it is interrupted for lack of improvement by the early stopping callback we have just created. \n",
    "\n",
    "For each epoch of training, performance will be tested on both the training set and on the previously created validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "201667d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 25s 19ms/step - loss: 1.6950 - accuracy: 0.3927 - val_loss: 1.4442 - val_accuracy: 0.4836\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 1.3772 - accuracy: 0.5076 - val_loss: 1.4466 - val_accuracy: 0.4933\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.2533 - accuracy: 0.5569 - val_loss: 1.2083 - val_accuracy: 0.5685\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1773 - accuracy: 0.5876 - val_loss: 1.1403 - val_accuracy: 0.5985\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1140 - accuracy: 0.6108 - val_loss: 1.0283 - val_accuracy: 0.6318\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.0699 - accuracy: 0.6265 - val_loss: 1.0026 - val_accuracy: 0.6449\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.0258 - accuracy: 0.6411 - val_loss: 0.9928 - val_accuracy: 0.6451\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.9891 - accuracy: 0.6535 - val_loss: 0.9977 - val_accuracy: 0.6537\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9645 - accuracy: 0.6641 - val_loss: 0.9593 - val_accuracy: 0.6693\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.9342 - accuracy: 0.6724 - val_loss: 0.9741 - val_accuracy: 0.6599\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9202 - accuracy: 0.6827 - val_loss: 0.9682 - val_accuracy: 0.6701\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.9001 - accuracy: 0.6867 - val_loss: 0.9781 - val_accuracy: 0.6625\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8790 - accuracy: 0.6931 - val_loss: 0.9299 - val_accuracy: 0.6696\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8579 - accuracy: 0.7006 - val_loss: 0.8909 - val_accuracy: 0.6925\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8437 - accuracy: 0.7091 - val_loss: 0.8723 - val_accuracy: 0.6988\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.8284 - accuracy: 0.7075 - val_loss: 1.0126 - val_accuracy: 0.6532\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8153 - accuracy: 0.7166 - val_loss: 0.8757 - val_accuracy: 0.6919\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.8085 - accuracy: 0.7187 - val_loss: 0.8431 - val_accuracy: 0.7058\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7964 - accuracy: 0.7241 - val_loss: 0.8812 - val_accuracy: 0.6902\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7851 - accuracy: 0.7267 - val_loss: 0.8237 - val_accuracy: 0.7146\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7773 - accuracy: 0.7320 - val_loss: 0.8162 - val_accuracy: 0.7171\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7600 - accuracy: 0.7358 - val_loss: 0.8630 - val_accuracy: 0.7019\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.7573 - accuracy: 0.7345 - val_loss: 0.8035 - val_accuracy: 0.7217\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7400 - accuracy: 0.7434 - val_loss: 0.7917 - val_accuracy: 0.7277\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7398 - accuracy: 0.7439 - val_loss: 0.7908 - val_accuracy: 0.7253\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7293 - accuracy: 0.7468 - val_loss: 0.8039 - val_accuracy: 0.7256\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7224 - accuracy: 0.7480 - val_loss: 0.7927 - val_accuracy: 0.7262\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.7079 - accuracy: 0.7513 - val_loss: 0.8457 - val_accuracy: 0.7071\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7072 - accuracy: 0.7539 - val_loss: 0.7708 - val_accuracy: 0.7356\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6955 - accuracy: 0.7576 - val_loss: 0.7884 - val_accuracy: 0.7271\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6910 - accuracy: 0.7581 - val_loss: 0.7789 - val_accuracy: 0.7365\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6852 - accuracy: 0.7606 - val_loss: 0.7723 - val_accuracy: 0.7357\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6790 - accuracy: 0.7652 - val_loss: 0.7701 - val_accuracy: 0.7343\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6686 - accuracy: 0.7679 - val_loss: 0.7605 - val_accuracy: 0.7357\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6673 - accuracy: 0.7687 - val_loss: 0.7596 - val_accuracy: 0.7352\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6618 - accuracy: 0.7711 - val_loss: 0.7728 - val_accuracy: 0.7347\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6598 - accuracy: 0.7720 - val_loss: 0.7734 - val_accuracy: 0.7316\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6497 - accuracy: 0.7724 - val_loss: 0.7801 - val_accuracy: 0.7326\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6438 - accuracy: 0.7757 - val_loss: 0.7640 - val_accuracy: 0.7380\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6349 - accuracy: 0.7806 - val_loss: 0.7617 - val_accuracy: 0.7399\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6292 - accuracy: 0.7796 - val_loss: 0.7610 - val_accuracy: 0.7385\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6282 - accuracy: 0.7817 - val_loss: 0.7622 - val_accuracy: 0.7329\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6254 - accuracy: 0.7821 - val_loss: 0.7754 - val_accuracy: 0.7354\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6184 - accuracy: 0.7858 - val_loss: 0.7538 - val_accuracy: 0.7459\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6117 - accuracy: 0.7873 - val_loss: 0.7333 - val_accuracy: 0.7454\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6116 - accuracy: 0.7857 - val_loss: 0.7435 - val_accuracy: 0.7458\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6076 - accuracy: 0.7883 - val_loss: 0.7496 - val_accuracy: 0.7468\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5992 - accuracy: 0.7914 - val_loss: 0.7402 - val_accuracy: 0.7494\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6018 - accuracy: 0.7880 - val_loss: 0.7649 - val_accuracy: 0.7419\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5945 - accuracy: 0.7942 - val_loss: 0.7740 - val_accuracy: 0.7353\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5935 - accuracy: 0.7942 - val_loss: 0.7390 - val_accuracy: 0.7452\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5910 - accuracy: 0.7943 - val_loss: 0.7569 - val_accuracy: 0.7433\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5794 - accuracy: 0.7976 - val_loss: 0.7460 - val_accuracy: 0.7496\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5841 - accuracy: 0.7974 - val_loss: 0.7668 - val_accuracy: 0.7397\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5751 - accuracy: 0.7967 - val_loss: 0.7262 - val_accuracy: 0.7553\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5715 - accuracy: 0.8008 - val_loss: 0.7239 - val_accuracy: 0.7542\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5696 - accuracy: 0.8004 - val_loss: 0.7621 - val_accuracy: 0.7427\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5664 - accuracy: 0.8004 - val_loss: 0.7314 - val_accuracy: 0.7525\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5717 - accuracy: 0.8036 - val_loss: 0.7348 - val_accuracy: 0.7558\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5545 - accuracy: 0.8057 - val_loss: 0.7428 - val_accuracy: 0.7544\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5578 - accuracy: 0.8058 - val_loss: 0.7599 - val_accuracy: 0.7460\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5589 - accuracy: 0.8041 - val_loss: 0.7465 - val_accuracy: 0.7480\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5547 - accuracy: 0.8069 - val_loss: 0.7259 - val_accuracy: 0.7521\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5588 - accuracy: 0.8070 - val_loss: 0.7672 - val_accuracy: 0.7430\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5492 - accuracy: 0.8082 - val_loss: 0.7671 - val_accuracy: 0.7385\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.5405 - accuracy: 0.8126 - val_loss: 0.7152 - val_accuracy: 0.7623\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5458 - accuracy: 0.8079 - val_loss: 0.7325 - val_accuracy: 0.7537\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5418 - accuracy: 0.8111 - val_loss: 0.7566 - val_accuracy: 0.7473\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5342 - accuracy: 0.8148 - val_loss: 0.7757 - val_accuracy: 0.7375\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5384 - accuracy: 0.8122 - val_loss: 0.7570 - val_accuracy: 0.7465\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5346 - accuracy: 0.8135 - val_loss: 0.7355 - val_accuracy: 0.7528\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5276 - accuracy: 0.8161 - val_loss: 0.7889 - val_accuracy: 0.7437\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5323 - accuracy: 0.8136 - val_loss: 0.7216 - val_accuracy: 0.7589\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5245 - accuracy: 0.8161 - val_loss: 0.7468 - val_accuracy: 0.7514\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5201 - accuracy: 0.8187 - val_loss: 0.7341 - val_accuracy: 0.7583\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5220 - accuracy: 0.8185 - val_loss: 0.7308 - val_accuracy: 0.7605\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5190 - accuracy: 0.8177 - val_loss: 0.7669 - val_accuracy: 0.7463\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5131 - accuracy: 0.8214 - val_loss: 0.7498 - val_accuracy: 0.7518\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5158 - accuracy: 0.8194 - val_loss: 0.7427 - val_accuracy: 0.7560\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5145 - accuracy: 0.8199 - val_loss: 0.7280 - val_accuracy: 0.7592\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5119 - accuracy: 0.8227 - val_loss: 0.7283 - val_accuracy: 0.7598\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5099 - accuracy: 0.8220 - val_loss: 0.7321 - val_accuracy: 0.7558\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5106 - accuracy: 0.8214 - val_loss: 0.7343 - val_accuracy: 0.7575\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5063 - accuracy: 0.8252 - val_loss: 0.7163 - val_accuracy: 0.7625\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4956 - accuracy: 0.8268 - val_loss: 0.7264 - val_accuracy: 0.7549\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4955 - accuracy: 0.8279 - val_loss: 0.7328 - val_accuracy: 0.7554\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4963 - accuracy: 0.8255 - val_loss: 0.7766 - val_accuracy: 0.7520\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4952 - accuracy: 0.8298 - val_loss: 0.7596 - val_accuracy: 0.7544\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4953 - accuracy: 0.8247 - val_loss: 0.7124 - val_accuracy: 0.7606\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4927 - accuracy: 0.8281 - val_loss: 0.7291 - val_accuracy: 0.7658\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4850 - accuracy: 0.8309 - val_loss: 0.7559 - val_accuracy: 0.7429\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4820 - accuracy: 0.8330 - val_loss: 0.7452 - val_accuracy: 0.7566\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4860 - accuracy: 0.8319 - val_loss: 0.7400 - val_accuracy: 0.7551\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4882 - accuracy: 0.8291 - val_loss: 0.7156 - val_accuracy: 0.7626\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4781 - accuracy: 0.8313 - val_loss: 0.7356 - val_accuracy: 0.7563\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4781 - accuracy: 0.8334 - val_loss: 0.7517 - val_accuracy: 0.7516\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4777 - accuracy: 0.8345 - val_loss: 0.7450 - val_accuracy: 0.7525\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4781 - accuracy: 0.8346 - val_loss: 0.7306 - val_accuracy: 0.7558\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4733 - accuracy: 0.8346 - val_loss: 0.7626 - val_accuracy: 0.7436\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4768 - accuracy: 0.8311 - val_loss: 0.7421 - val_accuracy: 0.7627\n"
     ]
    }
   ],
   "source": [
    "# building and fitting a CNN\n",
    "\n",
    "CNN = models.Sequential()\n",
    "CNN.add(layers.Conv2D(128, 2, 2, activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Conv2D(256, 2, 2, activation=\"relu\"))\n",
    "CNN.add(layers.MaxPool2D(2))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Conv2D(128, 2, 2, activation=\"relu\"))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Flatten())\n",
    "CNN.add(layers.Dense(256, activation=\"relu\"))\n",
    "CNN.add(layers.Dropout(.2))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Dense(128, activation=\"relu\"))\n",
    "CNN.add(layers.Dropout(.1))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Dense(128, activation=\"relu\"))\n",
    "CNN.add(layers.Dropout(.1))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Dense(64, activation=\"relu\"))\n",
    "CNN.add(layers.BatchNormalization())\n",
    "CNN.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "CNN.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = CNN.fit(aug_train, validation_data=aug_val, epochs=100, shuffle=False, verbose=1, callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "088d6414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network reached a maximum training accuracy of 83.46% at epoch 98 and a maximum validation accuracy of 76.58% at epoch 90.\n"
     ]
    }
   ],
   "source": [
    "# printing best scores\n",
    "\n",
    "max_acc = max(history.history[\"accuracy\"]) \n",
    "max_val_acc = max(history.history[\"val_accuracy\"]) \n",
    "\n",
    "print(f\"The network reached a maximum training accuracy of {round(max_acc * 100, 2)}% \\\n",
    "at epoch {history.history['accuracy'].index(max_acc) + 1} and a maximum validation accuracy of \\\n",
    "{round(max_val_acc * 100, 2)}% at epoch {history.history['val_accuracy'].index(max_val_acc) + 1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65dd84",
   "metadata": {},
   "source": [
    "Finally, after training the network we evaluate its performance on the test set: this will give us a definitive idea of how well the network performs when it is presented with image data that has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c94b1e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.7433 - accuracy: 0.7609\n",
      "\n",
      "The network scored 76.09% accuracy on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Evaluating network performance on the test set\n",
    "\n",
    "scores = CNN.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nThe network scored {round(scores[1] * 100, 2)}% accuracy on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2751a9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The CNN that we built can correctly label the pictures from the CIFAR-10 dataset more than 75% of the time. There was no strong signal of *overfitting* during training, which means that by building a more complex network a better score could be obtained; however, this would require more computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
